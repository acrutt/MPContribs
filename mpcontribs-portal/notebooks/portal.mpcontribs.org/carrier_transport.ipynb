{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpcontribs.client import Client\n",
    "from time import sleep, time\n",
    "import gzip, json, os\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_public = True\n",
    "project = 'carrier_transport'\n",
    "client = Client('your-api-key-here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get and update project info (see https://portal.mpcontribs.org/carrier_transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.projects.get_entry(pk=project, _fields=['_all']).result()\n",
    "# client.projects.update_entry(pk=project, project={'long_title': 'Electronic Transport Properties'}).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create contributions with tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/project/projectdirs/matgen/fricci/transport_data/coarse'\n",
    "variables = [\n",
    "    {'key': 'cond_eff_mass', 'name': 'mₑᶜᵒⁿᵈ', 'unit': 'mₑ'},\n",
    "    {'key': 'seebeck_doping', 'name': 'S', 'unit': 'µV/K'},\n",
    "    {'key': 'cond_doping', 'name': 'σ', 'unit': '1/Ω/m/s'},\n",
    "]\n",
    "eigs_keys = ['ε₁', 'ε₂', 'ε₃', 'ε̄']\n",
    "props = {\n",
    "    'seebeck_doping': ['S', 'µV/K'],\n",
    "    'cond_doping': ['σ', '1/Ω/m/s'],\n",
    "    'kappa_doping': ['κₑ', 'W/K/m/s']\n",
    "}\n",
    "pfkey = '⟨S²σ⟩'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [x for x in os.scandir(input_dir) if x.is_file()]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def chunks(data, SIZE=500):\n",
    "    it = iter(data)\n",
    "    for i in range(0, len(data), SIZE):\n",
    "        if isinstance(data, dict):\n",
    "            yield {k: data[k] for k in islice(it, SIZE)}\n",
    "        else:\n",
    "            yield data[i:i+SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributions, existing = {}, []\n",
    "batch_size = 750\n",
    "total = len(files)\n",
    "\n",
    "for idx, obj in enumerate(files):\n",
    "    if not idx%1000:\n",
    "        print(idx, len(contributions))               \n",
    "\n",
    "    if len(contributions) >= batch_size or idx == total-1:\n",
    "        for i, chunk in enumerate(chunks(contributions, SIZE=250)):\n",
    "            contribs = [c['contrib'] for c in chunk.values()]\n",
    "            created = client.contributions.create_entries(contributions=contribs).result()\n",
    "            print(i, created['count'], 'contributions created')    \n",
    "\n",
    "            create_tables = []\n",
    "            for contrib in created['data']:\n",
    "                identifier = contrib['identifier']\n",
    "                for t in chunk[identifier]['tables']:\n",
    "                    t['contribution'] = contrib['id']\n",
    "                    create_tables.append(t)\n",
    "\n",
    "            print('submit', len(create_tables), 'tables ...')\n",
    "            for j, subchunk in enumerate(chunks(create_tables, SIZE=100)):\n",
    "                created = client.tables.create_entries(tables=subchunk).result()\n",
    "                print(j, created['count'], 'tables created')\n",
    "\n",
    "        contributions.clear()\n",
    "        existing.clear()\n",
    "    \n",
    "    if not len(contributions) and not len(existing):\n",
    "        has_more = True\n",
    "        while has_more:\n",
    "            skip = len(existing)\n",
    "            contribs = client.contributions.get_entries(\n",
    "                project=project, _skip=skip, _limit=250, _fields=['identifier']\n",
    "            ).result()\n",
    "            existing += [c['identifier'] for c in contribs['data']]\n",
    "            has_more = contribs['has_more']\n",
    "\n",
    "        print(len(existing), 'already uploaded.')\n",
    "\n",
    "    identifier = obj.name.split('.', 1)[0].rsplit('_', 1)[-1]\n",
    "\n",
    "    valid = bool(identifier.startswith('mp-') or identifier.startswith('mvc-'))\n",
    "    if not valid:\n",
    "        print(idx, identifier, 'not valid')\n",
    "        continue\n",
    "    if identifier in existing:\n",
    "        continue\n",
    "    if identifier in contributions:\n",
    "        print(idx, identifier, 'already parsed')\n",
    "        continue\n",
    "        \n",
    "    with gzip.open(obj.path, 'rb') as input_file:\n",
    "        data = json.loads(input_file.read())\n",
    "        contrib = {'project': project, 'identifier': identifier, 'is_public': is_public, 'data': {}}\n",
    "        task_type = list(data['gap'].keys())[0]\n",
    "        gap = list(data['gap'].values())[0]\n",
    "        contrib['data']['task'] = list(data['task_id'].values())[0]\n",
    "        contrib['data']['type'] = task_type\n",
    "        contrib['data']['metal'] = 'Yes' if gap < 0.1 else 'No'        \n",
    "        contrib['data']['T'] = '300 K'\n",
    "        contrib['data']['doplvl'] = '1e18 cm⁻³'\n",
    "        contrib['data']['ΔE'] = ' '.join([str(gap), 'eV'])\n",
    "        contrib['data']['V'] = ' '.join([str(data['volume']), 'Å³'])\n",
    "        \n",
    "        S2 = None\n",
    "        for v in variables:\n",
    "            for doping_type in ['p', 'n']:\n",
    "                d = data[task_type][v['key']].get(doping_type, {}).get('300', {}).get('1e+18', {})\n",
    "                \n",
    "                if d:\n",
    "                    eigs = d if isinstance(d, list) else d['eigs']\n",
    "                    key = '|'.join([v['name'], doping_type])\n",
    "                    contrib['data'][key] = dict(\n",
    "                        (eigs_keys[neig], ' '.join([str(eig), v['unit']]))\n",
    "                        for neig, eig in enumerate(eigs)\n",
    "                    )\n",
    "                    contrib['data'][key][eigs_keys[-1]] = ' '.join([str(np.mean(eigs)), v['unit']])\n",
    "                    if v['key'] == 'seebeck_doping':\n",
    "                        S2 = np.dot(d['tensor'], d['tensor'])\n",
    "                    elif v['key'] == 'cond_doping':\n",
    "                        pf = np.mean(np.linalg.eigh(np.dot(S2, d['tensor']))[0]) * 1e-8\n",
    "                        if pfkey not in contrib['data']:\n",
    "                            contrib['data'][pfkey] = {}\n",
    "                        contrib['data'][pfkey][doping_type] = ' '.join([str(pf), 'µW/cm/K²/s'])\n",
    "                        \n",
    "        # build data and max values for seebeck, conductivity and kappa\n",
    "        tables = []       \n",
    "        for prop_name, (label, unit) in props.items():\n",
    "            for doping_type in ['p', 'n']:\n",
    "                prop = data[task_type][prop_name][doping_type]\n",
    "                prop_averages, dopings, columns = [], None, ['T [K]']\n",
    "                temps = sorted(map(int, prop.keys()))\n",
    "                for temp in temps:\n",
    "                    row = [temp]\n",
    "                    if dopings is None:\n",
    "                        dopings = sorted(map(float, prop[str(temp)].keys()))\n",
    "                    for doping in dopings:\n",
    "                        doping_str = f'{doping:.0e}'\n",
    "                        if len(columns) <= len(dopings):\n",
    "                            columns.append(f'{doping_str} cm⁻³ [{unit}]')\n",
    "                        eigs = prop[str(temp)][doping_str]['eigs']\n",
    "                        row.append(np.mean(eigs))\n",
    "                    prop_averages.append(row)\n",
    "                \n",
    "                table_name = f'{label}({doping_type})'\n",
    "                np_prop_averages = np.array(prop_averages)\n",
    "                df = DataFrame(np_prop_averages, columns=columns)\n",
    "                for col in df.columns:\n",
    "                    df[col] = df[col].astype(str)\n",
    "                table = df.to_dict(orient='split')\n",
    "                table.pop('index')\n",
    "                table['name'] = table_name\n",
    "                table['is_public'] = is_public\n",
    "                tables.append(table)\n",
    "\n",
    "                arr_prop_avg = np.array(np_prop_averages)[:,1:]\n",
    "                max_v = np.max(arr_prop_avg)\n",
    "                if prop_name[0] == 's' and doping_type == 'n':\n",
    "                    max_v = np.min(arr_prop_avg)\n",
    "                if prop_name[0] == 'k':\n",
    "                    max_v = np.min(arr_prop_avg)\n",
    "                arg_max = np.argwhere(arr_prop_avg==max_v)[0]\n",
    "\n",
    "                elabel = label + 'ᵉ'\n",
    "                edoping_type = 'ⁿ' if doping_type == 'n' else 'ᵖ'\n",
    "                contrib['data'][elabel] = {\n",
    "                    doping_type: ' '.join([str(max_v), unit]),\n",
    "                    f'T{edoping_type}': ' '.join([str(temps[arg_max[0]]), 'K']),\n",
    "                    f'c{edoping_type}': ' '.join([str(dopings[arg_max[1]]), 'cm⁻³']),\n",
    "                }\n",
    "                \n",
    "        contributions[identifier] = {'contrib': contrib, 'tables': tables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up (CAREFUL!)\n",
    "has_more = True\n",
    "while has_more:\n",
    "    resp = client.contributions.delete_entries(project=project, _limit=250).result()    \n",
    "    print(resp['count'], 'contributions deleted')\n",
    "    has_more = resp['has_more']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
