http://the-huck.com[Patrick Huck] (https://github.com/tschaume[tschaume]), Sep-09-2014 +
*Under Development*

:toc:
:toc-placement: manual

[options="compact"]
toc::[]

user-contributed data submissions
---------------------------------

2014-09-09: Feedback from Internal MP meeting (Under Discussion)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

generally: great proposal. +
fix issue of 'snl_id' and 'mp_id' synchronicity first. +
CSV format good for tabular data, separate out the provenance into yaml. +
use plot.ly for embed- and shareable graphs, synchronize formats. +
avoid bibtex keys, store bibtex in yaml provenance file. +
model script similar to AWS CLI.

objective
~~~~~~~~~

The objective of the link:user_submission_test.py[`user_submission_test.py`]
script is to develop a test scenario for the submission of a multitude of
possible user data formats and their resulting JSON representation using
http://pandas.pydata.org['Pandas'].  'Pandas' already handles a variety of
different data table formats with index columns and header rows, and their
translation into the according python objects. The respective common language
is well established and allows for the convenient
http://pandas.pydata.org/pandas-docs/dev/io.html#csv-text-files[import of data
from csv] and other sources via a few options as well as the definition of a
set of
http://pandas.pydata.org/pandas-docs/dev/io.html#writing-json[orientations] and
http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html[out-types]
to translate data objects to JSON via `to_json()` or to python dicts via `to_dict()`.
http://pandas.pydata.org/pandas-docs/stable/visualization.html[Default
plotting] of the data objects with sensible options are also provided. Basicly,
'Pandas' represents simple one-column, possibly indexed data tables via
`Series` and all else (i.e. even multi-indexed data) via `DataFrame` objects.
This considerably facilitates the programmatic submission of data using the
Materials Project's REST API for user and developer. The approach of using
'Pandas' for MP user data submissions would not only guarantee a common
language with already plenty of existing documentation but would also leave the
specific data table formats under the user's control.

user submissions in current MP infrastructure
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For the development of the current submission scheme we're working off the
assumption that each submission by the user is based on a unique 'snl_group_id'
(i.e., one csv-file per 'snl_group_id'). This allows for the extension of the
already existing 'projects' key in the SNL to serve as a list of projects
contributing to the respective SNL. Each element in this list would reference
the according document in the project's collection of data submissions. The
issue of mapping 'mp_id' and 'task_id' to 'snl_group_id' then needs to be
addressed separately. Note that the solution proposed here assumes the
submission of any general *final* user data associated with the respective SNL.
It does not try to solve the separate issue of a user's desire to submit
customized but MP-based user tasks to the MP's core task collection.

authors & publications
~~~~~~~~~~~~~~~~~~~~~~

The organization of authors and publications is long well established in the
scientific community using dedicated BibTeX files including designated field
names and entry types commonly required for references. GUIs & tools exist for
many platforms to maintain these file types such that the user does not need to
be familiar with the particular syntax. In the MP, each project would maintain
a single "global/project-wide" bibtex-file which would be submitted separately
from the data. The existing python module
http://pybtex.sourceforge.net/manual.html#using-pybtex-programmatically[Pybtex]
can be used to parse the bibtex-file and save it to the Mongo database. The
resulting bibtex-key would serve as a unique identifier to link the data in the
user submission to the corresponding authors and publications. The bibtex-keys
can then be resolved dynamically into author names etc. on the frontend, for
instance.

data submission format
~~~~~~~~~~~~~~~~~~~~~~

'Pandas' allows for the import of data from many different sources which makes
it a suitable basis to be extended later based on the feedback by MP's user
community. For the purpose of developing a test scenario of user submissions we
start with basic CSV files using a minimal amount of meta-data necessary to
customize the submission for MP. CSVs are commonly used, even ubiquitous! They
are easy to produce and parse, while well suited for tabular data. CSV
does not handle hierarchical data or free-form text well, but this should be
manageable for now. Once the general submission scheme is established, other
more programmatic ways of submission should be easily implementable.
link:input.csv[`input.csv`] is a csv-formatted file with a collection of
possible user data formats separated in nested sections by multiples of `>`.
The character chosen as separator is open for discussion. See inline comments
in the following excerpt from `input.csv` for more info on the details of the
input format.

[source,bash]
--------------
>>>> GENERAL
# - anything after section delimiter is parsed as section name, excl. comments
# - number of '>'s denotes section level (depth), min. 3, max. 6
# - a general section with properties, settings and defaults. The MP might
#   require certain unique row names in this section (snl-id, mp-id, xtal-name..)
snl-group-id: 12345
# comment lines in (sub)section body are ignored
xtal-name: Al2O3
submitters: slany@nrel.gov, pgraf@nrel.gov # usernames = email addresses
references: slany14, slany12 # bibtex-keys

>>>> CRYSTAL
>>> general
# - use colon as separator for 'general' and 'plot' (sub-)sections
# - simple list of key-value pairs
# - key serves as index -> needs to be unique
# - separate header entry in general section is not necessary. Pandas already
#   provides that since it is part of the data (user just "labels" the data)
publications: ja295760, ja295765 # bibtex-keys
standards: fere, gwvd
authors: nrel_authors # bibtex-key
comment: This dataset is the result of DOE grant 12345, NSF grant 12345, and the contributed efforts of many researchers. # line-wrapping?
>>> plot
# 'plot' subsection:
# - specify a plot and its options
# - supports columns to be plotted referred to by header name
# - key-value pairs in this section are passed through to df.plot() (not tested)
x: alpha
>>> data
# - 'data' sections are parsed with comma as delimiter
# - always require header row in data section
# - define column header like desired for axis labels (for now)
alpha,beta,gamma
10,11,12

>>>> BAND GAPS
# a section with a simple list of annotated numbers including units. The number
# can have multiple columns to provide info on the respective conditions under
# which the number was generated, for instance.
>>> plot
x: name
kind: bar
>>> data
name,type,functional,method,value,unit
band gap,indirect,GLLB-SC,Kohn-Sham,6.887038,eV
band gap,direct,GLLB-SC,Kohn-Sham,6.886986,eV
deriv. discont.,,GLLB-SC,,2.42833,eV

>>>> ELASTIC TENSOR
# no subsections -> parsed as 'data'
Matrix,Exp.,Theo.,Ref.
c11,287.0,284.7,PSP11 # bibtex-key
c22,302.1,299.5

>>>> DIELECTRIC CONSTANT
>>> plot #  no y-axis headers -> overlay all y_i vs x in plot
x: freq
>>> data
freq,real,imag
0,2.0065,0
--------------

data import code
~~~~~~~~~~~~~~~~

The `RecursiveParser` recursively splits the input file section-by-section
using appropriate regular expressions with the current separator level. When
the minimum section level is reached, the section body is read into 'Pandas'
objects `Series` or `DataFrame` via `read_csv()` and subsequently incorporated
into the output document with the appropriate nesting using `to_dict()`.
Lists of 1-1-mappings are always imported as an indexed `Series` object
("squeezed"). For the `Series` object, the conversion to a dict is obvious. For
the `DataFrame` object, `list` is used as conversion type if all columns are
numeric and `records` for all else. The `RecursiveDict` class extends usual
python dicts for nested updating. The `plot` function reads the data from
`output.json` and produces 'Pandas' default plots. It currently only passes the
key-value pairs of the 'plot' subsections through to 'Pandas' without checks or
secondary adjustments.

features to be implemented in the future
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

define separator in (sub-)section title line +
support multiple plots on same data, implement/check df.plot() options +
comments: no special line breaks, allow for wrapping +
special syntax for multi-index tables, args in DataFrame.read_csv()? +
support optional indentation +
input/data validation and error handling +
parsing authors/publications based on project-wide bibtex-file +
use section names as keywords to facilitate search feature +
use XMCD data provided by ALS (Alpha N'Diaye) to test submission scheme

JSON-formatted data for MongoDB & Pandas Plots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Running link:user_submission_test.py[`user_submission_test.py`] over
link:input.csv[`input.csv`], pretty-prints the imported data using 'Pandas'
defaults and outputs a JSON representation of how the data would be saved in
MP's database internally (-> link:output.json[`output.json`]). Finally, the
imported data is plotted using 'Pandas' defaults based on the generated
`output.json`.

Band Gaps
^^^^^^^^^

[options="header",cols=",a"]
|=======================================================
| Pandas Plot | JSON Representation
| image:png/band_gaps.png[width="400px"] |
-------------
{
  ...
  "band gaps": {
    "data": [
      {
        "functional": "GLLB-SC", 
        "method": "Kohn-Sham", 
        "name": "band gap", 
        "type": "indirect", 
        "unit": "eV", 
        "value": 6.887038
      }, 
      ...
    ], 
    "plot": {
      "kind": "bar", 
      "x": "name"
    }
  }, 
  ...
}
-------------
|=======================================================

Elastic Tensor
^^^^^^^^^^^^^^

[options="header",cols=",a"]
|=======================================================
| Pandas Plot | JSON Representation
| image:png/elastic_tensor.png[width="400px"] |
----------------
{
  ...
  "elastic tensor": [
    {
      "Exp.": 287.0, 
      "Matrix": "c11", 
      "Ref.": "PSP11 ", 
      "Theo.": 284.7
    }, 
    ...
  ], 
  ...
}
----------------
|=======================================================

Dielectric Constants
^^^^^^^^^^^^^^^^^^^^

[options="header",cols=",a"]
|=======================================================
| Pandas Plot | JSON Representation
| image:png/dielectric_constant.png[width="400px"] |
----------------
{
  ...
  "dielectric constant": {
    "data": {
      "freq": [ 0.0, 0.5, 1.0, ... ], 
      "imag": [ 0.0, 0.0, 0.0, ... ],
      "real": [ 2.0065, 2.0073, 2.0097, ... ]
    }, 
    "plot": {
      "x": "freq"
    }
  }, 
  ...
}
----------------
|=======================================================

// vim: set syntax=asciidoc:
